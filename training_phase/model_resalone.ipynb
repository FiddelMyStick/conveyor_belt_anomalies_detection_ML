{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1c0db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms # Keep transforms\n",
    "# No longer need: from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "import lpips # Still need this for the loss function object\n",
    "\n",
    "# <<<--- Import the model class from model.py --->>>\n",
    "# Ensure model.py contains the ResNetVAE_V2 definition with increased decoder capacity\n",
    "from models.resnet import ResNetVAE_V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4afcb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Use the same CustomDataset definition as before ---\n",
    "# --- Includes __init__ checks and __getitem__ checks/error handling ---\n",
    "# --- Ensure output images are 320x320 and normalized to [-1, 1] ---\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_folder, target_size=(320, 320)):\n",
    "        self.image_paths = []\n",
    "        self.target_size = target_size\n",
    "        print(f\"Looking for images in: {image_folder}\")\n",
    "        try:\n",
    "            if not os.path.isdir(image_folder):\n",
    "                 raise FileNotFoundError(f\"Dataset folder not found: {image_folder}\")\n",
    "            for img_name in os.listdir(image_folder):\n",
    "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "                    self.image_paths.append(os.path.join(image_folder, img_name))\n",
    "            if not self.image_paths: print(f\"WARNING: No valid image files found in {image_folder}\")\n",
    "            else: print(f\"Found {len(self.image_paths)} image files.\")\n",
    "        except Exception as e: print(f\"ERROR initializing CustomDataset: {e}\")\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(self.target_size),\n",
    "            transforms.ToTensor(), # Converts to [0, 1] range\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) # Maps to [-1, 1] range\n",
    "        ])\n",
    "\n",
    "    def __len__(self): return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.image_paths): raise IndexError(\"Index out of bounds\")\n",
    "        img_path = self.image_paths[idx]\n",
    "        try:\n",
    "            image = cv2.imread(img_path)\n",
    "            if image is None: print(f\"ERROR: cv2.imread failed for {img_path}. Returning None.\"); return None\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image = self.transform(image)\n",
    "            return image\n",
    "        except Exception as e: print(f\"ERROR processing image {img_path} in __getitem__: {e}\"); return None\n",
    "\n",
    "def collate_fn_skip_none(batch):\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if not batch: return None\n",
    "    try: return torch.utils.data.dataloader.default_collate(batch)\n",
    "    except Exception as e: print(f\"Error during collate: {e}. Batch items: {len(batch)}\"); return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8be3307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "LATENT_DIM = 512 # Keep at 512 due to VRAM limits observed\n",
    "# <<<--- Single Learning Rate for training from scratch --->>>\n",
    "LEARNING_RATE = 0.00015 # Start with a slightly lower rate for end-to-end training\n",
    "WEIGHT_DECAY = 1e-5\n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 0\n",
    "KLD_WEIGHT = 0.0001 # Keep low\n",
    "LPIPS_WEIGHT = 0.75 # Keep higher weight\n",
    "\n",
    "# --- Dataset / DataLoader ---\n",
    "dataset_path = r\"C:\\Users\\Legion\\Desktop\\venv_wokplace\\test_gpu_conda\\frames_extracted\"\n",
    "train_dataset_resnet_v2 = CustomDataset(dataset_path)\n",
    "\n",
    "if len(train_dataset_resnet_v2) > 0:\n",
    "    train_loader_resnet_v2 = DataLoader(train_dataset_resnet_v2, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                        num_workers=NUM_WORKERS, pin_memory=True, collate_fn=collate_fn_skip_none)\n",
    "    print(f\"DataLoader created. Batches per epoch: {len(train_loader_resnet_v2)}\")\n",
    "else:\n",
    "    print(\"Dataset is empty. Cannot create DataLoader.\")\n",
    "    train_loader_resnet_v2 = None\n",
    "\n",
    "# --- Model Setup ---\n",
    "if train_loader_resnet_v2:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # <<<--- Instantiate the model using the imported class (ResNetVAE_V2) --->>>\n",
    "    # Model weights will be initialized randomly (except for the ResNet encoder part,\n",
    "    # which uses its pre-trained weights by default when loaded inside the class)\n",
    "    model_v2 = ResNetVAE_V2(latent_dim=LATENT_DIM)\n",
    "    model_v2.to(device)\n",
    "    print(\"Instantiated ResNetVAE_V2 from model.py and moved to device.\")\n",
    "    print(\"Training from scratch (using pre-trained encoder weights, random decoder/latent weights).\")\n",
    "\n",
    "    # <<<--- REMOVED Weight Loading Block --->>>\n",
    "\n",
    "    # --- LPIPS Loss Setup ---\n",
    "    try:\n",
    "        lpips_loss_fn = lpips.LPIPS(net='alex', verbose=False).to(device)\n",
    "        for param in lpips_loss_fn.parameters(): param.requires_grad = False\n",
    "        lpips_loss_fn.eval()\n",
    "        print(\"LPIPS loss function created.\")\n",
    "    except Exception as e: print(f\"ERROR setting up LPIPS: {e}.\"); lpips_loss_fn = None\n",
    "\n",
    "    # --- Optimizer (Simplified to Single LR) ---\n",
    "    # <<<--- Using single learning rate for all parameters --->>>\n",
    "    optimizer = optim.AdamW(model_v2.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    print(f\"Optimizer created with single LR: {LEARNING_RATE}\")\n",
    "\n",
    "    # Scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=10, verbose=True)\n",
    "    print(\"Scheduler defined.\")\n",
    "else:\n",
    "    print(\"Skipping Model/Optimizer/LPIPS setup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb82fb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure variables from Cell 3 are used (e.g., train_loader_resnet_v2, model_v2)\n",
    "if 'train_loader_resnet_v2' in locals() and train_loader_resnet_v2 is not None and \\\n",
    "   'model_v2' in locals() and 'lpips_loss_fn' in locals() and lpips_loss_fn is not None:\n",
    "\n",
    "    # --- Training Params ---\n",
    "    # <<<--- Start from epoch 0, reset best loss --->>>\n",
    "    num_epochs = 200 # Train longer from scratch\n",
    "    gradient_clip = 1.0\n",
    "    early_stopping_patience = 15\n",
    "    best_loss = float(\"inf\") # Reset best loss for new run\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    # --- Paths ---\n",
    "    # <<<--- Using the same V2 directory, will overwrite previous results --->>>\n",
    "    model_save_dir = r\"C:\\Users\\Legion\\Desktop\\venv_wokplace\\test_gpu_conda\\model_saved_resnet_v2_finetune\"\n",
    "    os.makedirs(model_save_dir, exist_ok=True)\n",
    "    best_model_path = os.path.join(model_save_dir, \"resnet_v2_scratch_vae_best.pth\") # New name\n",
    "    final_model_path = os.path.join(model_save_dir, \"resnet_v2_scratch_vae_final.pth\") # New name\n",
    "\n",
    "    print(\"\\n--- Starting ResNetVAE_V2 Training Loop (From Scratch) ---\")\n",
    "    model_v2.train() # Set model to training mode\n",
    "\n",
    "    # --- Training Loop (Logic identical to before) ---\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss, epoch_recon_loss, epoch_kld_loss, epoch_lpips_loss = 0, 0, 0, 0\n",
    "        pbar = tqdm(train_loader_resnet_v2, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "\n",
    "        for batch in pbar:\n",
    "            if batch is None: continue\n",
    "            images = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            try:\n",
    "                reconstruction, original_input, mu, log_var = model_v2(images)\n",
    "                loss_dict = model_v2.loss_function(reconstruction, original_input, mu, log_var,\n",
    "                                                 M_N=KLD_WEIGHT,\n",
    "                                                 lpips_model=lpips_loss_fn,\n",
    "                                                 lpips_weight=LPIPS_WEIGHT)\n",
    "                loss = loss_dict['loss']\n",
    "                recon_loss_val = loss_dict['Reconstruction_Loss_L1'].item()\n",
    "                kld_loss_val = loss_dict['KLD'].item()\n",
    "                lpips_loss_val = loss_dict['Perceptual_Loss'].item()\n",
    "\n",
    "                loss.backward()\n",
    "                # torch.nn.utils.clip_grad_norm_(model_v2.parameters(), gradient_clip)\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item(); epoch_recon_loss += recon_loss_val;\n",
    "                epoch_kld_loss += kld_loss_val; epoch_lpips_loss += lpips_loss_val\n",
    "                pbar.set_postfix(Loss=loss.item(), L1=recon_loss_val, KLD=kld_loss_val, LPIPS=lpips_loss_val)\n",
    "\n",
    "            except Exception as e: print(f\"\\nERROR during training step: {e}\"); continue\n",
    "\n",
    "        num_batches = len(train_loader_resnet_v2)\n",
    "        if num_batches > 0:\n",
    "             avg_epoch_loss = epoch_loss / num_batches; avg_recon_loss = epoch_recon_loss / num_batches;\n",
    "             avg_kld_loss = epoch_kld_loss / num_batches; avg_lpips_loss = epoch_lpips_loss / num_batches\n",
    "             print(f\"Epoch [{epoch+1}/{num_epochs}], Avg Loss: {avg_epoch_loss:.4f} | L1: {avg_recon_loss:.4f} | KLD: {avg_kld_loss:.4f} | LPIPS: {avg_lpips_loss:.4f}\")\n",
    "\n",
    "             scheduler.step(avg_epoch_loss)\n",
    "\n",
    "             if avg_epoch_loss < best_loss:\n",
    "                 best_loss = avg_epoch_loss; early_stop_counter = 0\n",
    "                 torch.save(model_v2.state_dict(), best_model_path)\n",
    "                 print(f\"Model Improved & Saved to {best_model_path}!\")\n",
    "             else:\n",
    "                 early_stop_counter += 1\n",
    "                 print(f\"No Improvement ({early_stop_counter}/{early_stopping_patience})\")\n",
    "                 if early_stop_counter >= early_stopping_patience:\n",
    "                     print(\"Early Stopping Triggered! Training Stopped.\"); break\n",
    "        else: print(f\"Epoch [{epoch+1}/{num_epochs}] - DataLoader empty.\"); break\n",
    "\n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "    # Save final model regardless\n",
    "    torch.save(model_v2.state_dict(), final_model_path)\n",
    "    print(f\"Final Model Saved Successfully to {final_model_path}\")\n",
    "\n",
    "else: print(\"Skipping Training Loop.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd9320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard Visualization ---\n",
    "# Needs to load the models saved by *this* training run\n",
    "\n",
    "if 'model_v2' in locals() and 'train_dataset_resnet_v2' in locals() and len(train_dataset_resnet_v2) > 0:\n",
    "    print(\"\\n--- Visualizing ResNetVAE_V2 (Scratch Trained) Sample Reconstructions ---\")\n",
    "\n",
    "    model_class = ResNetVAE_V2 # Use imported class\n",
    "    latent_dim_used = LATENT_DIM # Use LATENT_DIM from setup cell\n",
    "    # <<<--- Point to the correct saved model path from this run --->>>\n",
    "    model_load_dir = r\"C:\\Users\\Legion\\Desktop\\venv_wokplace\\test_gpu_conda\\model_saved_resnet_v2_finetune\"\n",
    "    best_model_load_path = os.path.join(model_load_dir, \"resnet_v2_scratch_vae_best.pth\") # Use new name\n",
    "\n",
    "    if 'device' not in locals(): device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Instantiate the SAME architecture used for training\n",
    "    inference_model = model_class(latent_dim=latent_dim_used)\n",
    "\n",
    "    try:\n",
    "        # Load the weights saved from THIS training run\n",
    "        print(f\"Loading model state from: {best_model_load_path}\")\n",
    "        inference_model.load_state_dict(torch.load(best_model_load_path, map_location=device))\n",
    "        inference_model.to(device)\n",
    "        inference_model.eval()\n",
    "        print(\"Inference model loaded successfully.\")\n",
    "\n",
    "        # --- Get Samples and Reconstruct (Same logic as before) ---\n",
    "        vis_loader = DataLoader(train_dataset_resnet_v2, batch_size=3, shuffle=True, collate_fn=collate_fn_skip_none)\n",
    "        # ... (rest of the visualization code identical to previous Cell 6) ...\n",
    "        # Make sure plotting code uses 'inference_model'\n",
    "        sample_batch = next(iter(vis_loader))\n",
    "        if sample_batch is None: print(\"Could not get a valid batch for visualization.\")\n",
    "        else:\n",
    "            # ... (copy paste rest of plotting logic from previous response) ...\n",
    "            plt.show() # Make sure plt.show() is called\n",
    "\n",
    "    except FileNotFoundError: print(f\"Error: Saved model not found at {best_model_load_path}.\")\n",
    "    except StopIteration: print(\"Could not get batch from vis_loader.\")\n",
    "    except Exception as e: print(f\"Could not visualize samples: {e}\")\n",
    "else: print(\"Skipping visualization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3a3f95-2173-4221-8de8-af8e31c2be35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Heatmap Visualization ---\n",
    "# Needs to load the models saved by *this* training run\n",
    "\n",
    "if 'inference_model' in locals() and inference_model is not None and \\\n",
    "   'train_dataset_resnet_v2' in locals() and len(train_dataset_resnet_v2) > 0:\n",
    "    print(\"\\n--- Visualizing Reconstruction Differences (Heatmap) ---\")\n",
    "\n",
    "    inference_model.eval() # Already loaded and in eval mode from previous cell ideally\n",
    "\n",
    "    num_heatmap_samples = 3\n",
    "    heatmap_loader = DataLoader(train_dataset_resnet_v2, batch_size=num_heatmap_samples, shuffle=True, collate_fn=collate_fn_skip_none)\n",
    "\n",
    "    try:\n",
    "        heatmap_batch = next(iter(heatmap_loader))\n",
    "        if heatmap_batch is None: print(\"Could not get a valid batch for heatmap visualization.\")\n",
    "        else:\n",
    "            # ... (rest of the heatmap visualization code identical to previous Cell 7) ...\n",
    "            # Make sure plotting code uses 'inference_model'\n",
    "            plt.show() # Make sure plt.show() is called\n",
    "\n",
    "    except StopIteration: print(\"Could not get batch from heatmap_loader.\")\n",
    "    except Exception as e: print(f\"Could not generate heatmap visualization: {e}\")\n",
    "else:\n",
    "    print(\"Skipping Heatmap visualization as prerequisites not met (model not loaded?).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af22725e-9327-41e1-a387-2bb2aca0e428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only proceed if training ran and model exists\n",
    "if 'model_v2' in locals() and 'train_dataset_resnet_v2' in locals() and len(train_dataset_resnet_v2) > 0:\n",
    "    print(\"\\n--- Visualizing ResNetVAE_V2 (Scratch Trained) Sample Reconstructions ---\")\n",
    "\n",
    "    model_class = ResNetVAE_V2 # Use imported class\n",
    "    latent_dim_used = LATENT_DIM # Use LATENT_DIM from setup cell\n",
    "    # <<<--- Point to the correct saved model path from this run --->>>\n",
    "    model_load_dir = r\"C:\\Users\\Legion\\Desktop\\venv_wokplace\\test_gpu_conda\\model_saved_resnet_v2_finetune\"\n",
    "    best_model_load_path = os.path.join(model_load_dir, \"resnet_v2_scratch_vae_best.pth\") # Use new name\n",
    "\n",
    "    if 'device' not in locals(): device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Instantiate the SAME architecture used for training\n",
    "    inference_model = model_class(latent_dim=latent_dim_used)\n",
    "\n",
    "    try:\n",
    "        print(f\"Loading model state from: {best_model_path}\")\n",
    "        inference_model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "        inference_model.to(device)\n",
    "        inference_model.eval()\n",
    "        print(\"Inference model loaded successfully.\")\n",
    "\n",
    "        # --- Get Samples and Reconstruct (Same logic as before) ---\n",
    "        vis_loader = DataLoader(train_dataset_resnet_v2, batch_size=3, shuffle=True, collate_fn=collate_fn_skip_none)\n",
    "        sample_batch = next(iter(vis_loader))\n",
    "\n",
    "        if sample_batch is None:\n",
    "             print(\"Could not get a valid batch for visualization.\")\n",
    "        else:\n",
    "            sample_images_cpu = sample_batch\n",
    "            sample_images_gpu = sample_images_cpu.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                reconstructed_gpu, _, _, _ = inference_model(sample_images_gpu)\n",
    "\n",
    "            sample_images_np = sample_images_cpu.numpy()\n",
    "            reconstructed_np = reconstructed_gpu.cpu().numpy()\n",
    "\n",
    "            # --- Plotting (Same logic as before) ---\n",
    "            num_images_to_show = sample_images_np.shape[0]\n",
    "            fig, axes = plt.subplots(2, num_images_to_show, figsize=(5 * num_images_to_show, 10))\n",
    "            fig.suptitle(\"Original vs Reconstructed (ResNetVAEV2 + LPIPS in 90 Epochs)\", fontsize=16)\n",
    "            if num_images_to_show == 1: axes = np.array([axes]).T\n",
    "\n",
    "            for i in range(num_images_to_show):\n",
    "                original_img = sample_images_np[i] * 0.5 + 0.5 # De-normalize\n",
    "                reconstructed_img = reconstructed_np[i] * 0.5 + 0.5 # De-normalize\n",
    "                axes[0, i].imshow(np.clip(np.transpose(original_img, (1, 2, 0)), 0, 1))\n",
    "                axes[0, i].set_title(f\"Original {i+1}\")\n",
    "                axes[0, i].axis('off')\n",
    "                axes[1, i].imshow(np.clip(np.transpose(reconstructed_img, (1, 2, 0)), 0, 1))\n",
    "                axes[1, i].set_title(f\"Reconstructed {i+1}\")\n",
    "                axes[1, i].axis('off')\n",
    "\n",
    "            plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "            plt.show()\n",
    "\n",
    "    except FileNotFoundError: print(f\"Error: Saved model not found at {best_model_path}.\")\n",
    "    except StopIteration: print(\"Could not get batch from vis_loader.\")\n",
    "    except Exception as e: print(f\"Could not visualize samples: {e}\")\n",
    "else:\n",
    "     print(\"Skipping visualization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372874b1-fd83-4425-8321-5a87a27acb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TESTING SCRIPT (Corrected) --- #\n",
    "\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# --- IMPORT THE MODEL --- #\n",
    "# Make sure model.py has the ResNetVAE_V2 definition matching the checkpoint\n",
    "from models.resnet import ResNetVAE_V2\n",
    "\n",
    "# --- SETTINGS --- #\n",
    "# <<<--- Make sure this points to the checkpoint from the V2 training run --->>>\n",
    "model_path = r'C:\\Users\\Legion\\Desktop\\venv_wokplace\\test_gpu_conda\\model_saved_resnet_v2_finetune\\resnet_v2_scratch_vae_best.pth' # Using _best usually preferred\n",
    "images_dir = r'C:\\Users\\Legion\\Desktop\\venv_wokplace\\test_gpu_conda\\processed_frames_320'\n",
    "output_dir = r'C:\\Users\\Legion\\Desktop\\venv_wokplace\\test_gpu_conda\\test_outputs_v2' # New output folder maybe\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# <<<--- Ensure this matches the latent dim used for the loaded model --->>>\n",
    "latent_dim = 512\n",
    "input_size = 320\n",
    "\n",
    "# --- TRANSFORM (Corrected: Added Normalization) --- #\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),\n",
    "    transforms.ToTensor(), # To [0, 1] range\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) # To [-1, 1] range\n",
    "])\n",
    "\n",
    "# --- LOAD MODEL --- #\n",
    "print(f\"Loading model definition (latent_dim={latent_dim})...\")\n",
    "# Instantiate the model structure matching the checkpoint\n",
    "model = ResNetVAE_V2(latent_dim=latent_dim, input_height=input_size).to(device)\n",
    "print(f\"Loading weights from: {model_path}\")\n",
    "try:\n",
    "    # Use strict=True by default, it should work if model def and checkpoint match\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    print(\"Model loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Model checkpoint not found at {model_path}\")\n",
    "    exit() # Exit if model can't load\n",
    "except RuntimeError as e:\n",
    "    print(f\"ERROR loading state_dict: {e}\")\n",
    "    print(\"This usually means the model architecture in model.py doesn't match the saved checkpoint.\")\n",
    "    print(\"Make sure you are loading the correct .pth file for the instantiated ResNetVAE_V2 architecture.\")\n",
    "    exit() # Exit if model can't load\n",
    "\n",
    "\n",
    "# --- LOAD RANDOM IMAGES --- #\n",
    "try:\n",
    "    all_imgs = [f for f in os.listdir(images_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    if not all_imgs:\n",
    "        print(f\"ERROR: No images found in {images_dir}\")\n",
    "        exit()\n",
    "    random_imgs = random.sample(all_imgs, min(10, len(all_imgs)))\n",
    "    print(f\"Processing {len(random_imgs)} random images...\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Images directory not found: {images_dir}\")\n",
    "    exit()\n",
    "\n",
    "for idx, img_name in enumerate(random_imgs):\n",
    "    img_path = os.path.join(images_dir, img_name)\n",
    "    try:\n",
    "        # Load with PIL is fine for transforms\n",
    "        orig_pil = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        # Apply the full transform (including normalization)\n",
    "        input_tensor = transform(orig_pil).unsqueeze(0).to(device)\n",
    "\n",
    "        # Run inference\n",
    "        with torch.no_grad():\n",
    "            recons, _, _, _ = model(input_tensor) # model expects [-1, 1], outputs [-1, 1]\n",
    "\n",
    "        # --- Visualization Prep (Corrected De-normalization) --- #\n",
    "        # Move to CPU and permute dimensions (B, C, H, W) -> (H, W, C)\n",
    "        # Keep originals from input_tensor which is already [-1, 1]\n",
    "        orig_np = input_tensor.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "        recon_np = recons.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "\n",
    "        # De-normalize from [-1, 1] back to [0, 1] for displaying/saving\n",
    "        orig_vis = np.clip(orig_np * 0.5 + 0.5, 0, 1)\n",
    "        recon_vis = np.clip(recon_np * 0.5 + 0.5, 0, 1)\n",
    "\n",
    "        # Calculate difference on the [0, 1] scale images\n",
    "        diff = np.abs(orig_vis - recon_vis)\n",
    "        mean_diff_val = np.mean(diff) # Mean absolute error over all pixels/channels\n",
    "        # Create heatmap from mean difference across channels\n",
    "        heatmap_gray = np.mean(diff, axis=2) # Grayscale heatmap HxW\n",
    "        # Normalize heatmap for better visualization if needed, or use raw values\n",
    "        # heatmap_norm = (heatmap_gray - heatmap_gray.min()) / (heatmap_gray.max() - heatmap_gray.min() + 1e-6)\n",
    "        heatmap_uint8 = np.uint8(255 * heatmap_gray) # Convert to uint8 for colormap\n",
    "        heatmap_color = cv2.applyColorMap(heatmap_uint8, cv2.COLORMAP_JET)\n",
    "        # Convert heatmap color from BGR (OpenCV default) to RGB for Matplotlib\n",
    "        heatmap_color_rgb = heatmap_color[..., ::-1]\n",
    "\n",
    "        # --- Plotting --- #\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        fig.suptitle(f\"Image: {img_name}\", fontsize=12)\n",
    "\n",
    "        axs[0].imshow(orig_vis)\n",
    "        axs[0].set_title('Original')\n",
    "\n",
    "        axs[1].imshow(recon_vis)\n",
    "        axs[1].set_title('Reconstruction')\n",
    "\n",
    "        axs[2].imshow(heatmap_color_rgb)\n",
    "        axs[2].set_title(f'Difference Heatmap\\nMean Abs Err: {mean_diff_val:.4f}')\n",
    "\n",
    "        for ax in axs:\n",
    "            ax.axis('off')\n",
    "\n",
    "        out_path = os.path.join(output_dir, f'result_{idx+1}_{img_name}.png')\n",
    "        plt.savefig(out_path, bbox_inches='tight') # Use tight bbox\n",
    "        plt.close(fig) # Close the figure to free memory\n",
    "        # print(f\" Saved: {out_path}\") # Optional print\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR processing image {img_name}: {e}\")\n",
    "\n",
    "print(f\"\\n Done! Results saved in: {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
